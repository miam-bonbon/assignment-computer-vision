{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miam-bonbon/assignment-computer-vision/blob/main/assignment_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZwsy0c1GDp0"
      },
      "source": [
        "# Multi-label Image Classification of Fruits on a Platter\n",
        "\n",
        "My goal is to develop a deep learning model that can accurately identify and classify multiple fruits present in images of fruit platters.\n",
        "\n",
        "The [dataset](https://universe.roboflow.com/suratthani-rajabhat-university/rcnn-oumhw) consists of images of fruit platters with various fruits arranged on them. Each image can contain one or more fruits from a set of 6 classes, including an \"other\" class for objects that are not among the defined fruit categories.\n",
        "\n",
        "My approach applies theory I learned in the module \"Computer Vision\" by Susanne Suter and Marco Willi and implements best practices according to [Karpathy:19](http://karpathy.github.io/2019/04/25/recipe/).\n",
        "\n",
        "Google Colab GPUs: We start by using a A100 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKIAraQGinh"
      },
      "source": [
        "## Dataset analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYSJ86JPJcxM"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T37uEcrF1taB",
        "outputId": "721a41fa-0658-49cf-90c4-4dcc807c703f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dqL8n0k4eSo",
        "outputId": "eb14f15f-9871-4881-dddd-a80010a52dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Hh1GSOM1iV",
        "outputId": "7154e227-829c-41a4-83de-1edb70ce09ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/assignment-computer-vision\n",
            "total 16\n",
            "drwx------ 5 root root 4096 Nov 23 13:30 data\n",
            "-rw------- 1 root root 7733 Nov 23 13:33 plot_functions.py\n",
            "drwx------ 2 root root 4096 Nov 23 13:43 __pycache__\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/assignment-computer-vision\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cuDJB19-fZPA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "class print_style:\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    END = '\\033[0m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4ZtpoRlEmEDo"
      },
      "outputs": [],
      "source": [
        "import plot_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7wfrxSuNvaS",
        "outputId": "846e5ed4-f2f1-453d-e994-d1d3f2d25ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 23 20:34:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wxhhzR5IQ9qC"
      },
      "outputs": [],
      "source": [
        "# Dataset configuration\n",
        "base_path = 'data'\n",
        "num_classes = 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for further processing\n",
        "# Train\n",
        "train_image_dir = os.path.join(base_path, 'train')\n",
        "train_annotation_file = os.path.join(base_path, 'train/_annotations.csv')\n",
        "\n",
        "if not os.path.exists(train_annotation_file):\n",
        "    print(f\"Error: Annotation file not found at {train_annotation_file}\")\n",
        "\n",
        "train_annotations = pd.read_csv(train_annotation_file)\n",
        "train_image_files = glob.glob(os.path.join(train_image_dir, '*.jpg'))\n",
        "\n",
        "# TEst\n",
        "test_image_dir = os.path.join(base_path, 'test')\n",
        "test_annotation_file = os.path.join(base_path, 'test/_annotations.csv')\n",
        "\n",
        "if not os.path.exists(test_annotation_file):\n",
        "    print(f\"Error: Annotation file not found at {test_annotation_file}\")\n",
        "\n",
        "test_annotations = pd.read_csv(test_annotation_file)\n",
        "test_image_files = glob.glob(os.path.join(test_image_dir, '*.jpg'))\n",
        "\n",
        "# Validation\n",
        "validation_image_dir = os.path.join(base_path, 'valid')\n",
        "validation_annotation_file = os.path.join(base_path, 'valid/_annotations.csv')\n",
        "\n",
        "if not os.path.exists(validation_annotation_file):\n",
        "    print(f\"Error: Annotation file not found at {validation_annotation_file}\")\n",
        "\n",
        "validation_annotations = pd.read_csv(validation_annotation_file)\n",
        "validation_image_files = glob.glob(os.path.join(validation_image_dir, '*.jpg'))"
      ],
      "metadata": {
        "id": "o7jAAH0exvjZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crop train_annotations to 500 please\n",
        "\n",
        "train_annotations = train_annotations.head(500)\n",
        "validation_annotations = validation_annotations.head(500)\n",
        "\n",
        "len(validation_annotations), len(train_annotations)"
      ],
      "metadata": {
        "id": "7NPfTnpgyVIH",
        "outputId": "526262e2-564c-4109-c198-8341bbee2108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sZJRSbRTdvs"
      },
      "source": [
        "### Analyze the Data Qualitatively\n",
        "\n",
        "First we \"dive\" into the data to get an understanding of it. Let's get a sense of the quantities.\n",
        "\n",
        "*TODO: label distribution, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j9r1zMHhYEFd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LWQ0-VqHb2bz"
      },
      "outputs": [],
      "source": [
        "# show images with bounding boxes\n",
        "def show_images_with_bboxes(base_path, num_images=5, cols=2, shuffle=False, fontsize=37):\n",
        "    \"\"\"\n",
        "    Iterates through images, displays them in a grid, and overlays bounding boxes.\n",
        "    \"\"\"\n",
        "    if not train_image_files:\n",
        "        print(f\"Error: No images found in {train_image_dir}\")\n",
        "        return\n",
        "\n",
        "    if (shuffle):\n",
        "      random.shuffle(train_image_files) # Shuffle for random display\n",
        "    num_images = min(num_images, len(train_image_files))\n",
        "    rows = (num_images + cols -1) // cols\n",
        "\n",
        "    image = mpimg.imread(train_image_files[0])\n",
        "    max_width = image.shape[1]\n",
        "    max_height = image.shape[0]\n",
        "\n",
        "    # Dynamic divisor based on image dimensions\n",
        "    divisor = max(max_width, max_height) / 10  # Adjust 10 as needed\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=((max_width * cols) / divisor, (max_height * rows) / divisor))\n",
        "\n",
        "    # fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))\n",
        "\n",
        "    if rows == 1 and cols == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for i in range(num_images):\n",
        "      image_path = train_image_files[i]\n",
        "      image = mpimg.imread(image_path)\n",
        "      row = i // cols\n",
        "      col = i % cols\n",
        "      ax = axes[row, col] if rows > 1 else axes[col]\n",
        "      ax.imshow(image)\n",
        "\n",
        "      filename = os.path.basename(image_path)\n",
        "      bboxes = train_annotations[train_annotations['filename'] == filename]\n",
        "\n",
        "\n",
        "      # Add bounding boxes\n",
        "      for index, row in bboxes.iterrows():\n",
        "          xmin, ymin, xmax, ymax, label = row['xmin'], row['ymin'], row['xmax'], row['ymax'], row['class']\n",
        "          rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
        "          ax.add_patch(rect)\n",
        "          ax.text(xmin, ymin, label, color='r', fontsize=fontsize)  # Add label text\n",
        "\n",
        "      ax.axis('off') # Hide axis\n",
        "\n",
        "    # remove empty subplots\n",
        "    for j in range(i+1, rows*cols):\n",
        "      row = j // cols\n",
        "      col = j % cols\n",
        "      fig.delaxes(axes[row,col] if rows > 1 else axes[col])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nhPppz56da8G"
      },
      "outputs": [],
      "source": [
        "# show_images_with_bboxes(base_path, num_images=20, cols=4, shuffle=True, fontsize=37)"
      ]
    },
    {
      "source": [
        "def create_dataset(train_annotation_file, train_image_dir):\n",
        "  \"\"\"\n",
        "  Creates training dataset from annotation file and image directory.\n",
        "\n",
        "  Args:\n",
        "      train_annotation_file: A pandas DataFrame containing annotation data.\n",
        "      train_image_dir: The directory containing the images.\n",
        "\n",
        "  Returns:\n",
        "      dataset_images: A list of image arrays.\n",
        "      dataset_targets: A list of bounding box coordinates.\n",
        "      dataset_labels: A list of labels (1 for banana, 0 for others).\n",
        "  \"\"\"\n",
        "  dataset_images = []\n",
        "  dataset_targets = []\n",
        "  dataset_labels = []\n",
        "\n",
        "  for index, row in train_annotation_file.iterrows():\n",
        "      (filename, width, height, class_name, xmin, ymin, xmax, ymax) = row\n",
        "\n",
        "      dataset_image_fullpath = os.path.join(train_image_dir, filename)\n",
        "      dataset_img = tf.keras.preprocessing.image.load_img(dataset_image_fullpath, target_size=(height, width))\n",
        "      dataset_img_arr = tf.keras.preprocessing.image.img_to_array(dataset_img)\n",
        "\n",
        "      xmin = round(xmin/ width, 2)\n",
        "      ymin = round(ymin/ height, 2)\n",
        "      xmax = round(xmax/ width, 2)\n",
        "      ymax = round(ymax/ height, 2)\n",
        "\n",
        "      dataset_images.append(dataset_img_arr)\n",
        "      dataset_targets.append((xmin, ymin, xmax, ymax))\n",
        "      dataset_labels.append(1 if class_name == 'banana' else 0)\n",
        "\n",
        "  return dataset_images, dataset_targets, dataset_labels"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jeXyXubpdaWl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get training sets\n",
        "train_images, train_targets, train_labels = create_dataset(train_annotations, train_image_dir)\n",
        "\n",
        "# # Get test sets\n",
        "# test_images, test_targets, test_labels = create_dataset(test_annotations, test_image_dir)\n",
        "\n",
        "# Get validation sets\n",
        "validation_images, validation_targets, validation_labels = create_dataset(validation_annotations, validation_image_dir)"
      ],
      "metadata": {
        "id": "BCBdKoO7ftpA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "width = 640\n",
        "height = 640\n",
        "num_classes = 2\n",
        "# classes = [\"Circle\", \"No-Circle\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "rGX39nOsETuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "#create the common input layer\n",
        "input_shape = (height, width, 3)\n",
        "input_layer = tf.keras.layers.Input(input_shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5kyNTredETuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "# Build the model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "base_layers = layers.Rescaling(1./255, name='bl_1')(input_layer)\n",
        "base_layers = layers.Conv2D(16, 3, padding='same', activation='relu', name='bl_2')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_3')(base_layers)\n",
        "base_layers = layers.Conv2D(32, 3, padding='same', activation='relu', name='bl_4')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_5')(base_layers)\n",
        "base_layers = layers.Conv2D(64, 3, padding='same', activation='relu', name='bl_6')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_7')(base_layers)\n",
        "base_layers = layers.Flatten(name='bl_8')(base_layers)\n",
        "\n",
        "classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
        "classifier_branch = layers.Dense(num_classes, name='cl_head')(classifier_branch)\n",
        "\n",
        "locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
        "locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
        "locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
        "locator_branch = layers.Dense(4, activation='sigmoid', name='bb_head')(locator_branch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9ceb1TS9ETuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "model = tf.keras.Model(input_layer, outputs=[classifier_branch, locator_branch])"
      ],
      "outputs": [],
      "metadata": {
        "id": "dUOdjDJMETuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_1 (\u001b[38;5;33mRescaling\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_2 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m640\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m448\u001b[0m │ bl_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ bl_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_4 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m4,640\u001b[0m │ bl_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ bl_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_6 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m18,496\u001b[0m │ bl_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ bl_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_8 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m409600\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ bl_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_1 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m52,428,928\u001b[0m │ bl_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_2 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ bb_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cl_1 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m52,428,928\u001b[0m │ bl_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_3 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ bb_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cl_head (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m258\u001b[0m │ cl_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_head (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m132\u001b[0m │ bb_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ bl_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bl_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ bl_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bl_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ bl_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bl_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bl_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">409600</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bl_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">52,428,928</span> │ bl_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ bb_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cl_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">52,428,928</span> │ bl_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ bb_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cl_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ cl_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bb_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │ bb_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,892,166\u001b[0m (400.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,892,166</span> (400.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,892,166\u001b[0m (400.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,892,166</span> (400.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "2ezp7SfKETuL",
        "outputId": "44fa69cf-1be5-4ef0-d689-340b5fc6541f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "losses = {\"cl_head\":tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \"bb_head\":tf.keras.losses.MSE}\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "jb26A2RMETuM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "model.compile(loss=losses, optimizer='Adam', metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "sz3wOEzuETuM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "training_epochs = 20\n",
        "\n",
        "print(train_images[0].shape)\n",
        "print(train_targets[0])\n",
        "print(train_labels[0])\n",
        "train_images = np.array(train_images)\n",
        "train_targets = np.array(train_targets)\n",
        "train_labels = np.array(train_labels)\n",
        "print(train_images[0].shape)\n",
        "print(train_targets[0])\n",
        "print(train_labels[0])\n",
        "\n",
        "validation_images = np.array(validation_images)\n",
        "validation_targets = np.array(validation_targets)\n",
        "validation_labels = np.array(validation_labels)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(640, 640, 3)\n",
            "(0.25, 0.18, 0.59, 0.47)\n",
            "0\n",
            "(640, 640, 3)\n",
            "[0.25 0.18 0.59 0.47]\n",
            "0\n"
          ]
        }
      ],
      "metadata": {
        "id": "CMlj3wGSETuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bca2fd-d44b-4ded-9d7c-fc8cb039d272"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "trainTargets = {\n",
        "    \"cl_head\": train_labels,\n",
        "    \"bb_head\": train_targets\n",
        "}\n",
        "\n",
        "validationTargets = {\n",
        "    \"cl_head\": validation_labels,\n",
        "    \"bb_head\": validation_targets\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "JtKDIxa7ETuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "print(type(trainTargets))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ],
      "metadata": {
        "id": "lXGF3-HHETuN",
        "outputId": "5e3326cc-c6f2-49c8-bf06-bc3384bfe0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def data_generator(images, targets, labels, batch_size):\n",
        "    \"\"\"Generates batches of data for training.\n",
        "\n",
        "    Args:\n",
        "        images: List of image arrays.\n",
        "        targets: List of bounding box coordinates.\n",
        "        labels: List of labels.\n",
        "        batch_size: The size of each batch.\n",
        "\n",
        "    Yields:\n",
        "        A tuple of (image_batch, target_batch) for each batch.\n",
        "    \"\"\"\n",
        "    num_samples = len(images)\n",
        "    indices = list(range(num_samples))\n",
        "    while True:  # Loop indefinitely for multiple epochs\n",
        "        random.shuffle(indices)  # Shuffle data for each epoch\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            image_batch = [images[j] for j in batch_indices]\n",
        "            # Ensure target batch elements are at least rank 1 and have defined types\n",
        "            # Adjust the shape of 'cl_head' to match the model's output shape (e.g., [?, 4])\n",
        "            target_batch = {\n",
        "                # Reshape labels to (None, 4) before stacking\n",
        "                \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n",
        "                \"bb_head\": tf.stack([tf.cast(targets[j], dtype=tf.float32) for j in batch_indices])\n",
        "            }\n",
        "            yield np.array(image_batch), target_batch\n",
        "\n",
        "# ... (rest of your code)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0bsaxaYUxTHZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def data_generator(images, targets, labels, batch_size):\n",
        "  \"\"\"Generates batches of data for training.\n",
        "\n",
        "  Args:\n",
        "    images: List of image arrays.\n",
        "    targets: List of bounding box coordinates.\n",
        "    labels: List of labels.\n",
        "    batch_size: The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "    A tuple of (image_batch, target_batch) for each batch.\n",
        "  \"\"\"\n",
        "  num_samples = len(images)\n",
        "  indices = list(range(num_samples))\n",
        "  while True:  # Loop indefinitely for multiple epochs\n",
        "      random.shuffle(indices)  # Shuffle data for each epoch\n",
        "      for i in range(0, num_samples, batch_size):\n",
        "          batch_indices = indices[i:i + batch_size]\n",
        "          image_batch = [images[j] for j in batch_indices]\n",
        "          # Ensure target batch elements are at least rank 1 and have defined types\n",
        "          # Adjust the shape of 'cl_head' to match the model's output shape (e.g., [?, 4])\n",
        "          target_batch = {\n",
        "              # Reshape labels to (None, 4) before stacking\n",
        "              \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n",
        "              \"bb_head\": tf.stack([tf.cast(targets[j], dtype=tf.float32) for j in batch_indices])\n",
        "          }\n",
        "          yield np.array(image_batch), target_batch\n",
        "\n",
        "# Specify output signature for the data generator\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32), # Assuming your images are 640x640x3 and float32\n",
        "    {\n",
        "        \"cl_head\": tf.TensorSpec(shape=(None, 4), dtype=tf.int32),  # Adjust shape if necessary\n",
        "        \"bb_head\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32) # Adjust shape if necessary (assuming 4 coordinates for bounding box)\n",
        "    }\n",
        ")\n",
        "\n",
        "train_generator = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_images, train_targets, train_labels, batch_size=32),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "validation_generator = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(validation_images, validation_targets, validation_labels, batch_size=32),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_images) // 32,  # Number of batches per epoch\n",
        "    epochs=training_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_images) // 32\n",
        ")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UgWYGkopwrvU",
        "outputId": "9b927d0a-19a5-42ef-fa54-aa13d0addbe6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nInvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 1 values, but the requested shape has 4 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-33-ae33c62b490d>\", line 28, in data_generator\n    \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n\n  File \"<ipython-input-33-ae33c62b490d>\", line 28, in <listcomp>\n    \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n    return op(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 1 values, but the requested shape has 4 [Op:Reshape]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_503]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ae33c62b490d>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of batches per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nInvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 1 values, but the requested shape has 4 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-33-ae33c62b490d>\", line 28, in data_generator\n    \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n\n  File \"<ipython-input-33-ae33c62b490d>\", line 28, in <listcomp>\n    \"cl_head\": tf.stack([tf.cast(tf.reshape(labels[j], (1, 4)), dtype=tf.int32) for j in batch_indices]),\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n    return op(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 1 values, but the requested shape has 4 [Op:Reshape]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_503]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: please generate the correct model.fit for my data, thank you\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_images,\n",
        "    y=trainTargets,\n",
        "    epochs=training_epochs,\n",
        "    validation_data=(validation_images, validationTargets),\n",
        "    batch_size=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "HfYVG4w3tqN9",
        "outputId": "ab18d3b6-38e8-44cb-e3bd-ffeafd09c263"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_2, functional_1_1/cl_head_1/Add)' with input shapes: [10,4], [10,2].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-46e17aa9dfb4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: please generate the correct model.fit for my data, thank you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainTargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqueeze_or_expand_to_same_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_2, functional_1_1/cl_head_1/Add)' with input shapes: [10,4], [10,2]."
          ]
        }
      ]
    },
    {
      "source": [
        "def data_generator(images, targets, labels, batch_size):\n",
        "  \"\"\"Generates batches of data for training.\n",
        "\n",
        "  Args:\n",
        "    images: List of image arrays.\n",
        "    targets: List of bounding box coordinates.\n",
        "    labels: List of labels.\n",
        "    batch_size: The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "    A tuple of (image_batch, target_batch) for each batch.\n",
        "  \"\"\"\n",
        "  num_samples = len(images)\n",
        "  indices = list(range(num_samples))\n",
        "  while True:  # Loop indefinitely for multiple epochs\n",
        "    random.shuffle(indices)  # Shuffle data for each epoch\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      image_batch = [images[j] for j in batch_indices]\n",
        "      target_batch = {\n",
        "          \"cl_head\": [labels[j] for j in batch_indices],\n",
        "          \"bb_head\": [targets[j] for j in batch_indices]\n",
        "      }\n",
        "      yield np.array(image_batch), target_batch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8t2a9VgEvOZL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def data_generator(images, targets, labels, batch_size):\n",
        "  \"\"\"Generates batches of data for training.\n",
        "\n",
        "  Args:\n",
        "    images: List of image arrays.\n",
        "    targets: List of bounding box coordinates.\n",
        "    labels: List of labels.\n",
        "    batch_size: The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "    A tuple of (image_batch, target_batch) for each batch.\n",
        "  \"\"\"\n",
        "  num_samples = len(images)\n",
        "  indices = list(range(num_samples))\n",
        "  while True:  # Loop indefinitely for multiple epochs\n",
        "    random.shuffle(indices)  # Shuffle data for each epoch\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      image_batch = [images[j] for j in batch_indices]\n",
        "      # Ensure target batch elements are at least rank 1\n",
        "      target_batch = {\n",
        "          \"cl_head\": [np.expand_dims(labels[j], axis=0) for j in batch_indices], #  Ensure 'labels' is at least rank 1\n",
        "          \"bb_head\": [np.expand_dims(targets[j], axis=0) for j in batch_indices] #  Ensure 'targets' is at least rank 1\n",
        "      }\n",
        "      yield np.array(image_batch), target_batch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5Bq65RqJv9VI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def data_generator(images, targets, labels, batch_size):\n",
        "  \"\"\"Generates batches of data for training.\n",
        "\n",
        "  Args:\n",
        "    images: List of image arrays.\n",
        "    targets: List of bounding box coordinates.\n",
        "    labels: List of labels.\n",
        "    batch_size: The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "    A tuple of (image_batch, target_batch) for each batch.\n",
        "  \"\"\"\n",
        "  num_samples = len(images)\n",
        "  indices = list(range(num_samples))\n",
        "  while True:  # Loop indefinitely for multiple epochs\n",
        "    random.shuffle(indices)  # Shuffle data for each epoch\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      image_batch = [images[j] for j in batch_indices]\n",
        "      # Ensure target batch elements are at least rank 1 and have defined types\n",
        "      target_batch = {\n",
        "          \"cl_head\": tf.stack([tf.cast(np.expand_dims(labels[j], axis=0), dtype=tf.int32) for j in batch_indices]), # Stack and define type as tf.int32\n",
        "          \"bb_head\": tf.stack([tf.cast(np.expand_dims(targets[j], axis=0), dtype=tf.float32) for j in batch_indices]) # Stack and define type as tf.float32\n",
        "      }\n",
        "      yield np.array(image_batch), target_batch\n",
        "\n",
        "# Specify output signature for the data generator\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32), # Assuming your images are 640x640x3 and float32\n",
        "    {\n",
        "        \"cl_head\": tf.TensorSpec(shape=(None, 1), dtype=tf.int32),  # Adjust shape if necessary\n",
        "        \"bb_head\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32) # Adjust shape if necessary (assuming 4 coordinates for bounding box)\n",
        "    }\n",
        ")\n",
        "\n",
        "train_generator = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_images, train_targets, train_labels, batch_size=32),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "validation_generator = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(validation_images, validation_targets, validation_labels, batch_size=32),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_images) // 32,  # Number of batches per epoch\n",
        "    epochs=training_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_images) // 32\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "dQ7mf2Z5wT8A",
        "outputId": "153c7688-977c-4112-d543-93deb87542fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_2, functional_1/cl_head_1/Add)' with input shapes: [?,4], [?,2].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-af1b603fa978>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of batches per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqueeze_or_expand_to_same_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_2, functional_1/cl_head_1/Add)' with input shapes: [?,4], [?,2]."
          ]
        }
      ]
    },
    {
      "source": [
        "train_generator = data_generator(train_images, train_targets, train_labels, batch_size=32)  # Adjust batch_size as needed\n",
        "validation_generator = data_generator(validation_images, validation_targets, validation_labels, batch_size=32)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QEi2qKBzvNT_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_images) // 32,  # Number of batches per epoch\n",
        "    epochs=training_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_images) // 32\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "3gmXekaGvMS8",
        "outputId": "b3a66d38-6d47-4b08-92c4-ba309812b79e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "`output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6aebcafdc285>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of batches per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\u001b[0m in \u001b[0;36m_from_generator\u001b[0;34m(generator, output_types, output_shapes, args, output_signature, name)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTypeSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         raise TypeError(f\"`output_signature` must contain objects that are \"\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0;34mf\"subclass of `tf.TypeSpec` but found {type(spec)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         f\"which is not.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(train_images, trainTargets,\n",
        "                    validation_data=(validation_images, validationTargets),\n",
        "                    batch_size=4,\n",
        "                    epochs=training_epochs,\n",
        "                    shuffle=True,\n",
        "                    verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "scrolled": true,
        "id": "ah1JB7zLETuN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVWWhYjQtKfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOf7ZY9QvDeCohw7lPH/0yH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}